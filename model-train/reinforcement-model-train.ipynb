{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "\n",
    "'Process in Colab' if IN_COLAB else 'Process in Local'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    !pip install transformers\n",
    "    !pip install datasets\n",
    "    !pip install --upgrade accelerate\n",
    "    !pip install pfrl@git+https://github.com/voidful/pfrl.git\n",
    "    !pip install textrl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 깃허브에서는 빼야됨\n",
    "%cd drive/MyDrive/projects/ClauseSummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import datetime\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "import gc\n",
    "import logging\n",
    "from pprint import pprint\n",
    "from tqdm.notebook import tqdm\n",
    "logging.basicConfig(level=logging.INFO, stream=sys.stdout, format='%(asctime)s %(message)s', datefmt='%m-%d %H:%M')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.optim as optim\n",
    "from torch.optim import AdamW, SGD\n",
    "from torch.nn import MSELoss\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from datasets import load_from_disk, load_dataset, Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, LongformerTokenizer, AutoModel, AutoModelForSeq2SeqLM\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "import pfrl\n",
    "from textrl import TextRLEnv, TextRLActor, train_agent_with_evaluation\n",
    "import logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_newline_before_number(text:str) -> str: #숫자. 형태로 되어있는것에 개행문자를 추가.\n",
    "    text = re.sub(r'(\\d+)\\.',r'\\n\\1.', str(text))\n",
    "    return text\n",
    "\n",
    "def change_it_to_a_comma(text:str): # (1) (2) 형태를 ,으로\n",
    "    items = re.split(r'\\(\\d+\\)', text)\n",
    "    if len(items) > 1:\n",
    "        items[1] = items[0]+items[1]\n",
    "        del items[0]\n",
    "    return ','.join(items)\n",
    "\n",
    "def remove_whitespace_after_str(text:str):\n",
    "    text = re.sub(r\"\\b갑\\s\", r'갑', text)\n",
    "    text = re.sub(r\"\\b을\\s\", r'을', text)\n",
    "    text = re.sub(r\"\\b병\\s\", r'병', text)\n",
    "    text = re.sub(r\"\\b정\\s\", r'정', text)\n",
    "    return text\n",
    "\n",
    "def change_number_point(text:str): # 1. 2. 등을 제 1조 2 항 등으로 바꿔줌\n",
    "    items = re.split(r'\\d+\\.', text)\n",
    "    if len(items) > 1:\n",
    "        items[1] = items[0]+items[1]\n",
    "        del items[0]\n",
    "    return ''.join(items)\n",
    "\n",
    "def summary_preprocessing_func(text: str):\n",
    "    text = add_newline_before_number(text)\n",
    "    text = change_it_to_a_comma(text)\n",
    "    text = remove_whitespace_after_str(text)\n",
    "    text = change_number_point(text)\n",
    "    return text\n",
    "\n",
    "def text_preprocessing_func(text):\n",
    "    return re.sub(r'\\n[\\n ]+', '\\n', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(row: Dict[str, str]):\n",
    "    text = row['text']\n",
    "    summary = row['summary']\n",
    "    text = text_preprocessing_func(text)\n",
    "    summary = summary_preprocessing_func(summary)\n",
    "\n",
    "    return {'text': text, \n",
    "            'summary': summary\n",
    "            }\n",
    "\n",
    "def df_preprocessing(df: pd.DataFrame):\n",
    "    text_df = df[['text', 'summary']]\n",
    "    text_df = text_df.apply(preprocessing, axis=1, result_type='expand')\n",
    "\n",
    "    df[['text', 'summary']] = text_df[['text', 'summary']]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelForRewardGeneration(nn.Module):\n",
    "    def __init__(self, encoder_path, hidden_size=256):\n",
    "        super(ModelForRewardGeneration, self).__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(encoder_path)\n",
    "        self.hidden_size = hidden_size\n",
    "        # TODO: head 설계\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(768, hidden_size, bias=False),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout1d(0.1),\n",
    "            nn.Linear(hidden_size, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None):\n",
    "        x = self.encoder(input_ids, attention_mask).pooler_output\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "def reference_reward_loss(reward, pred):\n",
    "    return - torch.log10(1 + torch.exp(-reward * pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MANUAL_TRAINING = True\n",
    "MANUAL_VALIDATION = True\n",
    "MID_CHECKPOINT_NUM = 2\n",
    "MID_PROCESS_PRINT_NUM = 35\n",
    "\n",
    "NUM_EPOCHS = 1\n",
    "MAX_TOKEN = 4096\n",
    "learning_rate = 2e-5\n",
    "decay = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_model_checkpoint = 'psyche/kolongformer-4096'\n",
    "reward_model_path = './model/230705-10 59'\n",
    "summary_model_checkpoint = 'KETI-AIR-Downstream/long-ke-t5-base-summarization'\n",
    "\n",
    "print(f'reward_model_checkpoint: {reward_model_checkpoint}\\nsummary_model_checkpoint: {summary_model_checkpoint}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dataset_path = './data/dataset-term.json'\n",
    "tokenized_dataset_path = f'./data/{summary_model_checkpoint.replace(\"/\", \"-\")}-tokenized-dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_STR = datetime.datetime.now().strftime('%y%m%d-%H:%M')\n",
    "model_save_path = f\"./model/{SAVE_STR}\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Tokenizer & Model checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_tokenizer = AutoTokenizer.from_pretrained(summary_model_checkpoint)\n",
    "reward_tokenizer = LongformerTokenizer.from_pretrained(reward_model_checkpoint)\n",
    "\n",
    "summary_model = AutoModelForSeq2SeqLM.from_pretrained(summary_model_checkpoint)\n",
    "reward_model = ModelForRewardGeneration(reward_model_checkpoint)\n",
    "reward_model.encoder = AutoModel.from_pretrained(reward_model_path + '-encoder-final')\n",
    "reward_model.head.load_state_dict(torch.load(reward_model_path + '-head-final.pt'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(original_dataset_path, encoding='utf-8')\n",
    "\n",
    "df['input'] = df['text']\n",
    "df = df.drop(columns=['text'], axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## finding the best parameters\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "summary_model.train()\n",
    "reward_model.eval()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "option = {\n",
    "    'max_new_tokens': MAX_TOKEN,\n",
    "    'truncation': True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SummaryRLEnv(TextRLEnv):\n",
    "    def get_reward(self, input_item, predicted_list, finish):\n",
    "        reward = [0]\n",
    "        if finish:\n",
    "            tokenized_text = reward_tokenizer(input_item['text'], **option)\n",
    "            tokenized_summary = reward_tokenizer(predicted_list[0], **option)\n",
    "\n",
    "            tokenized_total_text = dict()\n",
    "            for key in tokenized_text:\n",
    "                if len(tokenized_text['input_ids']) + len(tokenized_summary['input_ids']) < MAX_TOKEN:\n",
    "                    tokenized_total_text[key] = tokenized_text[key] + tokenized_summary[key]\n",
    "                else:\n",
    "                    tokenized_total_text[key] = (tokenized_text[key][:- len(tokenized_summary['input_ids'])]\n",
    "                                                 + tokenized_summary[key]\n",
    "                    )\n",
    "                tokenized_total_text[key] = (tokenized_total_text[key] \n",
    "                                             + ([1] * (MAX_TOKEN - len(tokenized_total_text[key])))\n",
    "                )\n",
    "            reward = [float(reward_model(**tokenized_total_text).squeeze()) * 10]\n",
    "        return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SummaryRLEnv(summary_model, summary_tokenizer, observation_input=df, compare_sample=len(df))\n",
    "actor = TextRLActor(env, summary_model, summary_tokenizer)\n",
    "agent = actor.agent_ppo(update_interval=100, minibatch_size=3, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_agent_with_evaluation(\n",
    "    agent=agent,\n",
    "    env=env,\n",
    "    steps=300,\n",
    "    eval_n_steps=None,\n",
    "    eval_n_episodes=1,\n",
    "    train_max_episode_len=300,\n",
    "    eval_interval=10,\n",
    "    outdir=model_save_path,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
