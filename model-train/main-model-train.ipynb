{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "\n",
    "'Process in Colab' if IN_COLAB else 'Process in Local'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    !pip install transformers\n",
    "    !pip install datasets\n",
    "    !pip install evaluate\n",
    "    !pip install rouge_score\n",
    "    !pip install torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import datetime\n",
    "import os\n",
    "import gc\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorboard\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau, CosineAnnealingLR, CyclicLR\n",
    "import torchmetrics\n",
    "\n",
    "from datasets import load_dataset, load_from_disk, concatenate_datasets, DatasetDict, Dataset\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from transformers import BartConfig, T5Config\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "import evaluate\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge import Rouge\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizeMapWrapper:\n",
    "    def __init__(self, tokenizer, feature, option=None):\n",
    "        if option is None:\n",
    "            option = {\n",
    "                'max_length': 512,\n",
    "                'truncation': True,\n",
    "                'padding': 'max_length',\n",
    "            }\n",
    "        \n",
    "        self.feature = feature\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __call__(self, row):\n",
    "        return self.tokenizer(row[self.feature], **self.option)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'{self.__class__.__name__}(tokenizer={self.tokenizer})'\n",
    "\n",
    "class Seq2SeqTokenizeMapWrapper(TokenizeMapWrapper):\n",
    "    def __init__(self, tokenizer, feature, target, option=None):\n",
    "        super().__init__(tokenizer, feature, option)\n",
    "        self.target = target\n",
    "\n",
    "    def seq2seq_tokenize(self, row):\n",
    "        form_embeddings = self.tokenizer(row[self.feature], **self.option)\n",
    "        with self.tokenizer.as_target_tokenizer():\n",
    "            correct_form_embeddings = self.tokenizer(row[self.target], **self.option)\n",
    "\n",
    "        return {\n",
    "            'input_ids': form_embeddings['input_ids'],\n",
    "            'attention_mask': form_embeddings['attention_mask'],\n",
    "            'labels': correct_form_embeddings['input_ids'],\n",
    "        }\n",
    "\n",
    "    def __call__(self, row):\n",
    "        return self.seq2seq_tokenize(row)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting\n",
    "\n",
    "- 학습 환경에 맞게 조정하기 (특히 **경로 설정**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MANUAL_TRAINING = True\n",
    "MANUAL_VALIDATION = True\n",
    "NUM_EPOCHS = 1\n",
    "MID_CHECKPOINT_NUM = 2\n",
    "MID_PROCESS_PRINT_NUM = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_large_summary_checkpoint = 'lcw99/t5-large-korean-text-summary'\n",
    "t5_base_summary_checkpoint = 'eenzeenee/t5-base-korean-summarization'\n",
    "kobart_summary_checkpoint = 'gogamza/kobart-summarization'\n",
    "checkpoint = t5_large_summary_checkpoint\n",
    "print(f'Using Checkpoint: {checkpoint}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dataset_path = './data/dataset-term-summary.json'\n",
    "tokenized_dataset_path = f'./data/{checkpoint.replace(\"/\", \"-\")}-tokenized-dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_STR = datetime.datetime.now().strftime('%y%m%d-%H:%M')\n",
    "model_save_path = f\"./model/{SAVE_STR}\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Tokenizer & Model Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'bart' in checkpoint.lower():\n",
    "    config = BartConfig.from_pretrained(checkpoint)\n",
    "    #config['vocab'] = 30000\n",
    "else:\n",
    "    config = T5Config.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, \n",
    "                                          max_length=512, \n",
    "                                          truncation=False, \n",
    "                                          padding='max_length',\n",
    "                                          #vocab=config.vocab_size\n",
    "                                          )\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(tokenizer) != model.config.vocab_size:\n",
    "    raise RuntimeError(f'Tokenizer vocab size and model vocab size do not match(Tokenizer:{len(tokenizer)} Model: {model.config.vocab_size}). Which would lead to further error in training.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(tokenized_dataset_path):\n",
    "    dataset = Dataset.from_pandas(pd.read_json(original_dataset_path[['text', 'summary']], encoding='utf-8'))\n",
    "    tokenizer_wrapper = Seq2SeqTokenizeMapWrapper(tokenizer, 'text', 'summary')\n",
    "\n",
    "    tokenized_dataset = (dataset\n",
    "                         .map(tokenizer_wrapper, \n",
    "                              batched=True, \n",
    "                              batch_size=128, \n",
    "                              num_proc=10\n",
    "                              )\n",
    "                         .remove_columns(['text', 'summary'])\n",
    "                         )\n",
    "    \n",
    "    tokenized_dataset_dict = tokenized_dataset.train_test_split(test_size=0.2, shuffle=True)\n",
    "    tokenized_dataset_dict.save_to_disk(tokenized_dataset_path)\n",
    "else:\n",
    "    tokenized_dataset_dict = load_from_disk(tokenized_dataset_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    weight_decay=0.01,\n",
    "    report_to=\"tensorboard\",\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset_dict['train'],\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "\n",
    "if not MANUAL_TRAINING:\n",
    "    trainer.train()\n",
    "else:\n",
    "    total_loss = []\n",
    "    epoch_loss = []\n",
    "    batch_loss = []\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    trainset = tokenized_dataset_dict['train'].with_format('torch', device=device)\n",
    "    dataloader = DataLoader(trainset, batch_size=1, shuffle=False) # TODO: Batch size 조절\n",
    "    \n",
    "    # TODO: Write a code for **Hyperparameter Tuning**\n",
    "    optimizer = pass\n",
    "    scheduler = pass\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        total_steps = len(dataloader)\n",
    "        save_divisor = total_steps // MID_CHECKPOINT_NUM\n",
    "        print_divisor = total_steps // MID_PROCESS_PRINT_NUM\n",
    "        for i, batch in enumerate(tqdm(dataloader)):\n",
    "            X = {\n",
    "                    'input_ids': batch['input_ids'],\n",
    "                    'attention_mask': batch['attention_mask'],\n",
    "                }\n",
    "            y = batch['labels']\n",
    "            \n",
    "            outputs = model(**X, labels=y)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            scheduler.step()\n",
    "\n",
    "            batch_loss.append(loss.item())\n",
    "            if i % print_divisor == print_divisor - 1:\n",
    "                epoch_loss += batch_loss\n",
    "                batch_loss_series = pd.Series(batch_loss)\n",
    "                print(f'\\tbatch {i}\\tloss: {loss.item()}\\tmean: {batch_loss_series.mean()}')\n",
    "                batch_loss = []\n",
    "\n",
    "            if i % save_divisor == save_divisor - 1:\n",
    "                trainer.create_model_card(\n",
    "                    language='Korean',\n",
    "                    tags='Grammar',\n",
    "                    finetuned_from=checkpoint\n",
    "                )\n",
    "                trainer.save_model(model_save_path + f'-epoch-{epoch + 1}' + '-batch-{i + 1}')\n",
    "\n",
    "        total_loss += epoch_loss\n",
    "        batch_loss_series = pd.Series(epoch_loss)\n",
    "        epoch_loss = []\n",
    "        print(f'epoch {epoch + 1} loss: {loss.item()} mean: {batch_loss_series.mean()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_loss_series = pd.Series(total_loss)\n",
    "total_loss_series.plot.line()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOW_STR = datetime.datetime.now().strftime('%y%m%d-%H:%M')\n",
    "trainer.create_model_card(\n",
    "    language='Korean',\n",
    "    tags='Grammar',\n",
    "    #model='KoGrammar',\n",
    "    finetuned_from=checkpoint\n",
    ")\n",
    "trainer.save_model(f\"drive/MyDrive/projects/KoGrammar/models/{NOW_STR}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
