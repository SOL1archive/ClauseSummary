{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "  import google.colab\n",
        "  IN_COLAB = True\n",
        "except:\n",
        "  IN_COLAB = False\n",
        "\n",
        "'Process in Colab' if IN_COLAB else 'Process in Local'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-MLrhgyfoGWx",
        "outputId": "83747b0c-0fa3-4902-c824-7df5920569e0"
      },
      "outputs": [],
      "source": [
        "if IN_COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 프로젝트 디렉토리로 이동: 경우에 맞게 설정\n",
        "%cd drive/MyDrive/projects/ClauseSummary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uj7G7wLgoasO",
        "outputId": "78a9d849-ac96-46f6-ee5d-43875499a4a6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "if IN_COLAB:\n",
        "    if not os.path.exists('trlx'):\n",
        "        !git clone https://github.com/CarperAI/trlx.git\n",
        "    %cd trlx\n",
        "    !pip install -e .\n",
        "    %cd ..\n",
        "    !pip install transformers\n",
        "    !pip install datasets\n",
        "    !pip install torchtyping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jsgFAO4_oQRj"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2023-07-07 13:28:14,625] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-07-07 13:28:15.570054: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-07-07 13:28:16.960006: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/wsl/lib:\n",
            "2023-07-07 13:28:16.960178: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/wsl/lib:\n",
            "2023-07-07 13:28:16.960186: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
          ]
        }
      ],
      "source": [
        "import datetime\n",
        "from typing import List\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as Fs\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "from datasets import load_from_disk, load_dataset, Dataset, DatasetDict\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModel, AutoModelForSeq2SeqLM\n",
        "\n",
        "import trlx\n",
        "from trlx.trlx import train\n",
        "from trlx.data.default_configs import (\n",
        "    ModelConfig,\n",
        "    OptimizerConfig,\n",
        "    PPOConfig,\n",
        "    SchedulerConfig,\n",
        "    TokenizerConfig,\n",
        "    TrainConfig,\n",
        "    TRLConfig,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TokenizeMapWrapper:\n",
        "    def __init__(self, tokenizer, feature, option=None):\n",
        "        if option is None:\n",
        "            option = {\n",
        "                'max_length': 4096,\n",
        "                'truncation': True,\n",
        "                'padding': 'max_length',\n",
        "            }\n",
        "\n",
        "        self.feature = feature\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __call__(self, row):\n",
        "        return self.tokenizer(row[self.feature], **self.option)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f'{self.__class__.__name__}(tokenizer={self.tokenizer})'\n",
        "\n",
        "class RewardTokenizeMapWrapper(TokenizeMapWrapper):\n",
        "    def __init__(self, tokenizer, text_feature, summary_feature, max_token=4096, prompt='summarization-num_lines-1: ', option=None):\n",
        "        if option is None:\n",
        "            option = {\n",
        "                'max_length': max_token,\n",
        "                'truncation': True,\n",
        "            }\n",
        "\n",
        "        self.prompt = prompt\n",
        "        self.max_token = option['max_length']\n",
        "        self.option = option\n",
        "        self.text_feature = text_feature\n",
        "        self.summary_feature = summary_feature\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __call__(self, row):\n",
        "        text = row[self.text_feature]\n",
        "        summary = row[self.summary_feature]\n",
        "\n",
        "        tokenized_text = self.tokenizer(text, **self.option)\n",
        "        tokenized_summary = self.tokenizer(summary, **self.option)\n",
        "        tokenized_total_text = dict()\n",
        "        for key in tokenized_text:\n",
        "            if len(tokenized_text['input_ids']) + len(tokenized_summary['input_ids']) < self.max_token:\n",
        "                tokenized_total_text[key] = tokenized_text[key] + tokenized_summary[key]\n",
        "            else:\n",
        "                tokenized_total_text[key] = (tokenized_text[key][:- len(tokenized_summary['input_ids'])]\n",
        "                                             + tokenized_summary[key]\n",
        "                )\n",
        "            tokenized_total_text[key] = (tokenized_total_text[key]\n",
        "                                         + ([1] * (self.max_token - len(tokenized_total_text[key])))\n",
        "            )\n",
        "        return tokenized_total_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tokenize_text_summary(text: str, summary: str, tokenizer, option=None):\n",
        "    if option is None:\n",
        "        option = {\n",
        "            'max_length': 4096,\n",
        "            'truncation': True,\n",
        "        }\n",
        "    max_token = option['max_length']\n",
        "\n",
        "    if text.startswith('summarization-num_lines-1: '):\n",
        "        text = text[len('summarization-num_lines-1: '):]\n",
        "    \n",
        "    tokenized_text = tokenizer(text, **option)\n",
        "    tokenized_summary = tokenizer(summary, **option)\n",
        "\n",
        "    tokenized_total_text = dict()\n",
        "    for key in tokenized_text:\n",
        "        if len(tokenized_text['input_ids']) + len(tokenized_summary['input_ids']) < max_token:\n",
        "            tokenized_total_text[key] = tokenized_text[key] + tokenized_summary[key]\n",
        "        else:\n",
        "            tokenized_total_text[key] = (tokenized_text[key][:- len(tokenized_summary['input_ids'])]\n",
        "                                         + tokenized_summary[key]\n",
        "            )\n",
        "        tokenized_total_text[key] = (tokenized_total_text[key]\n",
        "                                     + ([1] * (max_token - len(tokenized_total_text[key])))\n",
        "        )\n",
        "    return tokenized_total_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ModelForRewardGeneration(nn.Module):\n",
        "    def __init__(self, encoder_path, hidden_size=256):\n",
        "        super(ModelForRewardGeneration, self).__init__()\n",
        "        self.encoder = AutoModel.from_pretrained(encoder_path)\n",
        "        self.hidden_size = hidden_size\n",
        "        self.head1 = nn.Sequential(\n",
        "            nn.Linear(768, 1024, bias=False),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout1d(0.2),\n",
        "            nn.Linear(1024, 1024, bias=False),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout1d(0.2),\n",
        "            nn.Linear(1024, 512, bias=False),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout1d(0.1),\n",
        "            nn.Linear(512, hidden_size, bias=False),\n",
        "            nn.BatchNorm1d(hidden_size),\n",
        "            nn.GELU(),\n",
        "        )\n",
        "        self.head2 = nn.Sequential(\n",
        "            nn.Linear(hidden_size, 1),\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids=None, attention_mask=None):\n",
        "        x = self.encoder(input_ids, attention_mask).pooler_output\n",
        "        x = self.head1(x)\n",
        "        x = self.head2(x)\n",
        "        return x\n",
        "\n",
        "    def representation_forward(self, input_ids=None, attention_mask=None):\n",
        "        x = self.encoder(input_ids, attention_mask).pooler_output\n",
        "        x = self.head1(x)\n",
        "        return x\n",
        "    \n",
        "    def load(self, model_path):\n",
        "        self.encoder = AutoModel.from_pretrained(model_path + '-encoder')\n",
        "        self.head1.load_state_dict(torch.load(model_path + '-head1.pt'))\n",
        "        self.head2.load_state_dict(torch.load(model_path + '-head2.pt'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "SAVE_STR = datetime.datetime.now().strftime('%y-%m-%d-%H:%M')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "original_dataset_path = './data/dataset-term.json'\n",
        "checkpoint = 'KETI-AIR-Downstream/long-ke-t5-base-summarization'\n",
        "\n",
        "reward_model_checkpoint = 'psyche/kolongformer-4096'\n",
        "reward_model_path = './model/230707-03:06'\n",
        "\n",
        "dataset_path = f'./data/dataset-term'\n",
        "tokenized_dataset_path = f'./data/{checkpoint.replace(\"/\", \"-\")}dataset-term-tokenized'\n",
        "model_save_path = f'./model/{SAVE_STR}-summary-model'"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "DIdwS2_oq8nL"
      },
      "source": [
        "### Loading Dataset, Tokenizers & Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 555
        },
        "id": "kEbb9jCUnaU7",
        "outputId": "86f1f8cf-41dd-4229-8d92-dad2bb6437d5"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n",
        "\n",
        "reward_tokenizer = AutoTokenizer.from_pretrained(reward_model_checkpoint)\n",
        "reward_model = ModelForRewardGeneration(reward_model_checkpoint, 128)\n",
        "reward_model.load(reward_model_path)\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_json(original_dataset_path)\n",
        "df['text'] = 'summarization-num_lines-1: ' + df['text'] + ' </s> '\n",
        "df = df[['text']]\n",
        "if not os.path.exists(dataset_path):\n",
        "    dataset = Dataset.from_pandas(df)\n",
        "    dataset.save_to_disk(dataset_path)\n",
        "else:\n",
        "    dataset = load_from_disk(dataset_path)\n",
        "\n",
        "dataset_dict = dataset.train_test_split(test_size=0.1, seed=42)\n",
        "# train_dataset, val_dataset = ###-dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_set = [sample[\"text\"] for sample in dataset_dict[\"train\"]]\n",
        "val_set = [sample[\"text\"] for sample in dataset_dict[\"test\"]]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "IxWvdXxjq4BK"
      },
      "source": [
        "### PPO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i6MhZiOanxFN"
      },
      "outputs": [],
      "source": [
        "default_config = TRLConfig(\n",
        "    train=TrainConfig(\n",
        "        seq_length=400,\n",
        "        epochs=100,\n",
        "        total_steps=100000,\n",
        "        batch_size=12,\n",
        "        checkpoint_interval=10000,\n",
        "        eval_interval=100,\n",
        "        pipeline=\"PromptPipeline\",\n",
        "        trainer=\"AcceleratePPOTrainer\",\n",
        "        save_best=False,\n",
        "    ),\n",
        "    model=ModelConfig(\n",
        "        model_path=checkpoint,\n",
        "        num_layers_unfrozen=-1,\n",
        "        model_arch_type=\"seq2seq\",\n",
        "    ),\n",
        "    tokenizer=TokenizerConfig(\n",
        "        tokenizer_path=checkpoint,\n",
        "        padding_side=\"right\",\n",
        "        truncation_side=\"right\",\n",
        "    ),\n",
        "    optimizer=OptimizerConfig(\n",
        "        name=\"adamw\",\n",
        "        kwargs={\n",
        "            \"lr\": 5.0e-5,\n",
        "            \"betas\": [0.9, 0.999],\n",
        "            \"eps\": 1.0e-8,\n",
        "            \"weight_decay\": 1.0e-6,\n",
        "        },\n",
        "    ),\n",
        "    scheduler=SchedulerConfig(\n",
        "        name=\"cosine_annealing\",\n",
        "        kwargs={\n",
        "            \"T_max\": 100000,\n",
        "            \"eta_min\": 5.0e-5,\n",
        "        },\n",
        "    ),\n",
        "    method=PPOConfig(\n",
        "        name=\"PPOConfig\",\n",
        "        num_rollouts=128,\n",
        "        chunk_size=12,\n",
        "        ppo_epochs=4,\n",
        "        init_kl_coef=0.05,\n",
        "        target=6,\n",
        "        horizon=10000,\n",
        "        gamma=0.99,\n",
        "        lam=0.95,\n",
        "        cliprange=0.2,\n",
        "        cliprange_value=0.2,\n",
        "        vf_coef=1,\n",
        "        scale_reward=None,\n",
        "        ref_mean=None,\n",
        "        ref_std=None,\n",
        "        cliprange_reward=10,\n",
        "        gen_kwargs={\n",
        "            \"max_new_tokens\": 400,\n",
        "            \"do_sample\": True,\n",
        "            \"top_k\": 0,\n",
        "            \"top_p\": 0.9,\n",
        "            \"eos_token_id\": -1,\n",
        "        },\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "reward_model = reward_model.to(device)\n",
        "reward_model.eval()\n",
        "\n",
        "def get_reward(samples: List[str]):\n",
        "    reward_lt = []\n",
        "    for sample in samples:\n",
        "        sample = {\n",
        "            'prompt': sample[:sample.find(' </s> ')],\n",
        "            'label': sample[sample.find(' </s> ') + len(' </s> '):]\n",
        "        }\n",
        "        tokenized_total_text = tokenize_text_summary(sample['prompt'], sample['label'])\n",
        "        score = reward_model(\n",
        "            input_ids=torch.tensor(tokenized_total_text['input_ids']).to(device),\n",
        "            attention_mask=torch.tensor(tokenized_total_text['attention_mask']).to(device)\n",
        "        )\n",
        "        reward_lt.append(score)\n",
        "    \n",
        "    rewards = torch.cat(reward_lt, dim=0)\n",
        "    return rewards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dzx3pZhnr2Co"
      },
      "outputs": [],
      "source": [
        "trainer = train(\n",
        "    prompts=train_set,\n",
        "    eval_prompts=val_set,\n",
        "    reward_fn=get_reward,\n",
        "    config=default_config,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer.create_model_card(\n",
        "    model_name='tosan-base',\n",
        "    finetuned_from=checkpoint\n",
        ")\n",
        "\n",
        "trainer.save_model(model_save_path)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
