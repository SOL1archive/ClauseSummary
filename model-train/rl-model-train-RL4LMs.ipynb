{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "\n",
    "'Process in Colab' if IN_COLAB else 'Process in Local'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd drive/MyDrive/projects/ClauseSummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    !pip install transformers\n",
    "    !pip install datasets\n",
    "    !pip install --upgrade accelerate\n",
    "    if not os.path.exists('RL4LMs'):\n",
    "        !git clone https://github.com/allenai/RL4LMs.git\n",
    "    %cd RL4LMs\n",
    "    !pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TOKEN = 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import datetime\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "import gc\n",
    "import logging\n",
    "from pprint import pprint\n",
    "from tqdm.notebook import tqdm\n",
    "logging.basicConfig(level=logging.INFO, stream=sys.stdout, format='%(asctime)s %(message)s', datefmt='%m-%d %H:%M')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.optim as optim\n",
    "from torch.optim import AdamW, SGD\n",
    "from torch.nn import MSELoss\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from datasets import load_from_disk, load_dataset, Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, LongformerTokenizer, AutoModel, AutoModelForSeq2SeqLM\n",
    "\n",
    "from rl4lms.envs.text_generation.observation import Observation\n",
    "from rl4lms.envs.text_generation.reward import RewardFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_newline_before_number(text:str) -> str: #숫자. 형태로 되어있는것에 개행문자를 추가.\n",
    "    text = re.sub(r'(\\d+)\\.',r'\\n\\1.', str(text))\n",
    "    return text\n",
    "\n",
    "def change_it_to_a_comma(text:str): # (1) (2) 형태를 ,으로\n",
    "    items = re.split(r'\\(\\d+\\)', text)\n",
    "    if len(items) > 1:\n",
    "        items[1] = items[0]+items[1]\n",
    "        del items[0]\n",
    "    return ','.join(items)\n",
    "\n",
    "def remove_whitespace_after_str(text:str):\n",
    "    text = re.sub(r\"\\b갑\\s\", r'갑', text)\n",
    "    text = re.sub(r\"\\b을\\s\", r'을', text)\n",
    "    text = re.sub(r\"\\b병\\s\", r'병', text)\n",
    "    text = re.sub(r\"\\b정\\s\", r'정', text)\n",
    "    return text\n",
    "\n",
    "def change_number_point(text:str): # 1. 2. 등을 제 1조 2 항 등으로 바꿔줌\n",
    "    items = re.split(r'\\d+\\.', text)\n",
    "    if len(items) > 1:\n",
    "        items[1] = items[0]+items[1]\n",
    "        del items[0]\n",
    "    return ''.join(items)\n",
    "\n",
    "def summary_preprocessing_func(text: str):\n",
    "    text = add_newline_before_number(text)\n",
    "    text = change_it_to_a_comma(text)\n",
    "    text = remove_whitespace_after_str(text)\n",
    "    text = change_number_point(text)\n",
    "    return text\n",
    "\n",
    "def text_preprocessing_func(text):\n",
    "    return re.sub(r'\\n[\\n ]+', '\\n', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(row: Dict[str, str]):\n",
    "    text = row['text']\n",
    "    summary = row['summary']\n",
    "    text = text_preprocessing_func(text)\n",
    "    summary = summary_preprocessing_func(summary)\n",
    "\n",
    "    return {'text': text, \n",
    "            'summary': summary\n",
    "            }\n",
    "\n",
    "def df_preprocessing(df: pd.DataFrame):\n",
    "    text_df = df[['text', 'summary']]\n",
    "    text_df = text_df.apply(preprocessing, axis=1, result_type='expand')\n",
    "\n",
    "    df[['text', 'summary']] = text_df[['text', 'summary']]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelForRewardGeneration(nn.Module):\n",
    "    def __init__(self, encoder_path, hidden_size=256):\n",
    "        super(ModelForRewardGeneration, self).__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(encoder_path)\n",
    "        self.hidden_size = hidden_size\n",
    "        # TODO: head 설계\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(768, hidden_size, bias=False),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout1d(0.1),\n",
    "            nn.Linear(hidden_size, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None):\n",
    "        x = self.encoder(input_ids, attention_mask).pooler_output\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "def reference_reward_loss(reward, pred):\n",
    "    return - torch.log10(1 + torch.exp(-reward * pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SummaryRewardFunction(RewardFunction):\n",
    "    def __init__(self, reward_tokenizer, reward_model, option=None, *args) -> None:\n",
    "        super().__init__()\n",
    "        if option is None:\n",
    "            option = {\n",
    "                'max_new_tokens': MAX_TOKEN,\n",
    "                'truncation': True,\n",
    "            }\n",
    "        self.option = option\n",
    "        self.reward_tokenizer = reward_tokenizer\n",
    "        self.reward_model = reward_model\n",
    "\n",
    "   def __call__(self, prev_observation: Observation,\n",
    "                action: int,\n",
    "                current_observation: Observation,\n",
    "                done: bool,\n",
    "                meta_info: Dict[str, Any] = None) -> float:\n",
    "       if done:\n",
    "           tokenized_text = self.reward_tokenizer(input_item['text'], **self.option)\n",
    "            tokenized_summary = self.reward_tokenizer(predicted_list[0], **self.option)\n",
    "\n",
    "            tokenized_total_text = dict()\n",
    "            for key in tokenized_text:\n",
    "                if len(tokenized_text['input_ids']) + len(tokenized_summary['input_ids']) < MAX_TOKEN:\n",
    "                    tokenized_total_text[key] = tokenized_text[key] + tokenized_summary[key]\n",
    "                else:\n",
    "                    tokenized_total_text[key] = (tokenized_text[key][:- len(tokenized_summary['input_ids'])]\n",
    "                                                 + tokenized_summary[key]\n",
    "                    )\n",
    "                tokenized_total_text[key] = (tokenized_total_text[key] \n",
    "                                             + ([1] * (MAX_TOKEN - len(tokenized_total_text[key])))\n",
    "                )\n",
    "            reward = [float(self.reward_model(**tokenized_total_text).squeeze()) * 10]\n",
    "           \n",
    "           return reward\n",
    "       return 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting & Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_str = '''\n",
    "datapool:\n",
    "  id: cnn_daily_mail\n",
    "  args:\n",
    "    prompt_prefix: \"summarization-num_lines-4: \"\n",
    "tokenizer:\n",
    "  model_name: KETI-AIR-Downstream/long-ke-t5-base-summarization\n",
    "  padding_side: left\n",
    "  truncation_side: left\n",
    "  pad_token_as_eos_token: False\n",
    "  max_new_tokens: 4096\n",
    "  truncation: True\n",
    "env:\n",
    "  n_envs: 10\n",
    "  args:\n",
    "    max_prompt_length: 4096\n",
    "    max_episode_length: 450\n",
    "    terminate_on_eos: True\n",
    "    prompt_truncation_side: \"right\"\n",
    "    context_start_token: 0\n",
    "alg:\n",
    "  id: ppo\n",
    "  args: \n",
    "    n_steps: 512\n",
    "    batch_size: 64\n",
    "    verbose: 1\n",
    "    learning_rate: 2e-5\n",
    "    n_epochs: 5\n",
    "    ent_coef: 0.0\n",
    "  kl_div:\n",
    "    coeff: 0.001\n",
    "    target_kl: 0.2\n",
    "  policy:\n",
    "    id: seq2seq_lm_actor_critic_policy\n",
    "    args:\n",
    "      model_name: t5-base\n",
    "      apply_model_parallel: True\n",
    "      prompt_truncation_side: \"right\"\n",
    "      generation_kwargs:\n",
    "        do_sample: True\n",
    "        top_k: 50\n",
    "        min_length: 50\n",
    "        max_new_tokens: 100\n",
    "train_evaluation:\n",
    "  eval_batch_size: 100\n",
    "  n_iters: 100\n",
    "  eval_every: 10\n",
    "  generation_kwargs: \n",
    "    do_sample: True\n",
    "    top_k: 0\n",
    "    temperature: 0.7\n",
    "    min_length: 50\n",
    "    max_new_tokens: 100\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_model_checkpoint = 'psyche/kolongformer-4096'\n",
    "reward_model_path = './model/230705-10 59'\n",
    "summary_model_checkpoint = 'KETI-AIR-Downstream/long-ke-t5-base-summarization'\n",
    "\n",
    "print(f'reward_model_checkpoint: {reward_model_checkpoint}\\nsummary_model_checkpoint: {summary_model_checkpoint}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dataset_path = './data/dataset-term.json'\n",
    "tokenized_dataset_path = f'./data/{summary_model_checkpoint.replace(\"/\", \"-\")}-tokenized-dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_STR = datetime.datetime.now().strftime('%y%m%d-%H:%M')\n",
    "model_save_path = f\"./model/{SAVE_STR}\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load tokenizer & Model checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_tokenizer = AutoTokenizer.from_pretrained(summary_model_checkpoint)\n",
    "reward_tokenizer = LongformerTokenizer.from_pretrained(reward_model_checkpoint)\n",
    "\n",
    "summary_model = AutoModelForSeq2SeqLM.from_pretrained(summary_model_checkpoint)\n",
    "reward_model = ModelForRewardGeneration(reward_model_checkpoint)\n",
    "reward_model.encoder = AutoModel.from_pretrained(reward_model_path + '-encoder-final')\n",
    "reward_model.head.load_state_dict(torch.load(reward_model_path + '-head-final.pt'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(original_dataset_path, encoding='utf-8')\n",
    "\n",
    "df['input'] = df['text']\n",
    "df = df.drop(columns=['text'], axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "summary_model.train()\n",
    "reward_model.eval()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
