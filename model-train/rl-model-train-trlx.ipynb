{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "  import google.colab\n",
        "  IN_COLAB = True\n",
        "except:\n",
        "  IN_COLAB = False\n",
        "\n",
        "'Process in Colab' if IN_COLAB else 'Process in Local'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-MLrhgyfoGWx",
        "outputId": "83747b0c-0fa3-4902-c824-7df5920569e0"
      },
      "outputs": [],
      "source": [
        "if IN_COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if IN_COLAB:\n",
        "    # 프로젝트 디렉토리로 이동: 경우에 맞게 설정\n",
        "    %cd drive/MyDrive/projects/ClauseSummary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uj7G7wLgoasO",
        "outputId": "78a9d849-ac96-46f6-ee5d-43875499a4a6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "if IN_COLAB:\n",
        "    !pip install transformers\n",
        "    !pip install datasets\n",
        "    !pip install torchtyping\n",
        "    !pip install wandb\n",
        "    !pip install git+https://github.com/CarperAI/trlx\n",
        "    !pip install peft"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jsgFAO4_oQRj"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "from typing import List\n",
        "import pickle\n",
        "import re\n",
        "import string\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as Fs\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "from datasets import load_from_disk, load_dataset, Dataset, DatasetDict\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModel, AutoModelForSeq2SeqLM\n",
        "from peft import LoraConfig\n",
        "from peft.utils.config import TaskType\n",
        "\n",
        "import trlx\n",
        "from trlx.trlx import train\n",
        "from trlx.data.default_configs import (\n",
        "    ModelConfig,\n",
        "    OptimizerConfig,\n",
        "    PPOConfig,\n",
        "    SchedulerConfig,\n",
        "    TokenizerConfig,\n",
        "    TrainConfig,\n",
        "    TRLConfig,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tokenize_text_summary(text: str, summary: str, tokenizer, option=None):\n",
        "    if option is None:\n",
        "        option = {\n",
        "            'max_length': 4096,\n",
        "            'truncation': True,\n",
        "        }\n",
        "    max_token = option['max_length']\n",
        "\n",
        "    if text.startswith('summarization-num_lines-4: '):\n",
        "        text = text[len('summarization-num_lines-4: '):]\n",
        "\n",
        "    tokenized_text = tokenizer(text, **option)\n",
        "    tokenized_summary = tokenizer(summary, **option)\n",
        "\n",
        "    tokenized_total_text = dict()\n",
        "    for key in tokenized_text:\n",
        "        if len(tokenized_text['input_ids']) + len(tokenized_summary['input_ids']) < max_token:\n",
        "            tokenized_total_text[key] = tokenized_text[key] + tokenized_summary[key]\n",
        "        else:\n",
        "            tokenized_total_text[key] = (tokenized_text[key][:- len(tokenized_summary['input_ids'])]\n",
        "                                         + tokenized_summary[key]\n",
        "            )\n",
        "        tokenized_total_text[key] = (tokenized_total_text[key]\n",
        "                                     + ([1] * (max_token - len(tokenized_total_text[key])))\n",
        "        )\n",
        "    return tokenized_total_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ModelForRewardGeneration(nn.Module):\n",
        "    def __init__(self, encoder_path, hidden_size=256):\n",
        "        super(ModelForRewardGeneration, self).__init__()\n",
        "        self.encoder = AutoModel.from_pretrained(encoder_path)\n",
        "        self.hidden_size = hidden_size\n",
        "        self.head1 = nn.Sequential(\n",
        "            nn.Linear(768, 1024, bias=False),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout1d(0.2),\n",
        "            nn.Linear(1024, 1024, bias=False),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout1d(0.2),\n",
        "            nn.Linear(1024, 512, bias=False),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout1d(0.1),\n",
        "            nn.Linear(512, hidden_size, bias=False),\n",
        "            nn.BatchNorm1d(hidden_size),\n",
        "            nn.GELU(),\n",
        "        )\n",
        "        self.head2 = nn.Sequential(\n",
        "            nn.Linear(hidden_size, 1),\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids=None, attention_mask=None):\n",
        "        x = self.encoder(input_ids, attention_mask).pooler_output\n",
        "        x = self.head1(x)\n",
        "        x = self.head2(x)\n",
        "        return x\n",
        "\n",
        "    def representation_forward(self, input_ids=None, attention_mask=None):\n",
        "        x = self.encoder(input_ids, attention_mask).pooler_output\n",
        "        x = self.head1(x)\n",
        "        return x\n",
        "\n",
        "    def load(self, model_path):\n",
        "        self.encoder = AutoModel.from_pretrained(model_path + '-encoder')\n",
        "        self.head1.load_state_dict(torch.load(model_path + '-head1.pt'))\n",
        "        self.head2.load_state_dict(torch.load(model_path + '-head2.pt'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "SAVE_STR = datetime.datetime.now().strftime('%y-%m-%d-%H:%M')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "preprocessed_dataset_existance = os.path.exists('./data/dataset-term-preprocessed.json')\n",
        "\n",
        "original_dataset_path = './data/dataset-term-preprocessed.json' if preprocessed_dataset_existance else './data/dataset-term.json'\n",
        "checkpoint = 'KETI-AIR-Downstream/long-ke-t5-base-summarization'\n",
        "\n",
        "reward_model_checkpoint = 'psyche/kolongformer-4096'\n",
        "reward_model_path = './model/230707-03:06'\n",
        "\n",
        "dataset_path = f'./data/dataset-term'\n",
        "model_save_path = f'./model/{SAVE_STR}-summary-model'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "MAX_EPOCH = 5\n",
        "TOTAL_STEPS = 100000\n",
        "MAX_SEQ_LEN = 4096\n",
        "LR = 2e-4"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "DIdwS2_oq8nL"
      },
      "source": [
        "### Loading Dataset, Tokenizers & Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 555
        },
        "id": "kEbb9jCUnaU7",
        "outputId": "86f1f8cf-41dd-4229-8d92-dad2bb6437d5"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "reward_tokenizer = AutoTokenizer.from_pretrained(reward_model_checkpoint)\n",
        "reward_model = ModelForRewardGeneration(reward_model_checkpoint, 128)\n",
        "reward_model.load(reward_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load dataset\n",
        "df = pd.read_json(original_dataset_path)\n",
        "df['text'] = 'summarization-num_lines-4: ' + df['text'] + '\\nSummary: '\n",
        "df = df[['text']]\n",
        "if not os.path.exists(dataset_path):\n",
        "    dataset = Dataset.from_pandas(df)\n",
        "    dataset.save_to_disk(dataset_path)\n",
        "else:\n",
        "    dataset = load_from_disk(dataset_path)\n",
        "\n",
        "dataset_dict = dataset.train_test_split(test_size=0.1, seed=42)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "IxWvdXxjq4BK"
      },
      "source": [
        "### PPO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i6MhZiOanxFN"
      },
      "outputs": [],
      "source": [
        "config = TRLConfig(\n",
        "    train=TrainConfig(\n",
        "        seq_length=MAX_SEQ_LEN,\n",
        "        epochs=MAX_EPOCH,\n",
        "        total_steps=TOTAL_STEPS,\n",
        "        batch_size=4,\n",
        "        checkpoint_interval=1000,\n",
        "        eval_interval=100,\n",
        "        pipeline=\"PromptPipeline\",\n",
        "        trainer=\"AcceleratePPOTrainer\",\n",
        "        save_best=True,\n",
        "    ),\n",
        "    model=ModelConfig(\n",
        "        model_path=checkpoint,\n",
        "        num_layers_unfrozen=-1,\n",
        "        model_arch_type=\"seq2seq\",\n",
        "    ),\n",
        "    tokenizer=TokenizerConfig(\n",
        "        tokenizer_path=checkpoint,\n",
        "        padding_side=\"right\",\n",
        "        truncation_side=\"right\",\n",
        "    ),\n",
        "    optimizer=OptimizerConfig(\n",
        "        name=\"adamw\",\n",
        "        kwargs={\n",
        "            \"lr\": LR,\n",
        "            \"betas\": [0.9, 0.999],\n",
        "            \"eps\": 1.0e-8,\n",
        "            \"weight_decay\": 1.0e-4,\n",
        "        },\n",
        "    ),\n",
        "    scheduler=SchedulerConfig(\n",
        "        name=\"linear\",\n",
        "        kwargs={\n",
        "\n",
        "        },\n",
        "    ),\n",
        "    method=PPOConfig(\n",
        "        name=\"PPOConfig\",\n",
        "        num_rollouts=128,\n",
        "        chunk_size=8,\n",
        "        ppo_epochs=MAX_EPOCH,\n",
        "        init_kl_coef=0.05,\n",
        "        target=6,\n",
        "        horizon=10000,\n",
        "        gamma=0.99,\n",
        "        lam=0.95,\n",
        "        cliprange=0.2,\n",
        "        cliprange_value=0.2,\n",
        "        vf_coef=1,\n",
        "        scale_reward=None,\n",
        "        ref_mean=None,\n",
        "        ref_std=None,\n",
        "        cliprange_reward=10,\n",
        "        gen_kwargs={\n",
        "            \"max_new_tokens\": 400,\n",
        "            \"do_sample\": True,\n",
        "            \"top_k\": 0,\n",
        "            \"top_p\": 0.9,\n",
        "            \"eos_token_id\": tokenizer.eos_token_id,\n",
        "        },\n",
        "    ),\n",
        ")\n",
        "\n",
        "config.model.peft_config = LoraConfig(\n",
        "    r=8,\n",
        "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.1,\n",
        "    target_modules=['q', 'v']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "reward_model = reward_model.to(device)\n",
        "reward_model.eval()\n",
        "\n",
        "def get_reward(samples: List[str], **kwargs):\n",
        "    reward_lt = []\n",
        "    for sample in samples:\n",
        "        sample = {\n",
        "            'prompt': sample[:sample.find('Summary: ')],\n",
        "            'output': sample[sample.find('Summary: ') + len('Summary: '):]\n",
        "        }\n",
        "        tokenized_total_text = tokenize_text_summary(sample['prompt'], sample['output'], reward_tokenizer)\n",
        "        with torch.no_grad():\n",
        "            score = reward_model(\n",
        "                input_ids=torch.tensor(tokenized_total_text['input_ids']).repeat(2, 1).to(device),\n",
        "                attention_mask=torch.tensor(tokenized_total_text['attention_mask']).repeat(2, 1).to(device)\n",
        "            )\n",
        "        reward_lt.append(score[0] * 10)\n",
        "\n",
        "    rewards = torch.cat(reward_lt, dim=0)\n",
        "    return rewards\n",
        "\n",
        "def get_prompt_dataset(prompts, max_length=MAX_SEQ_LEN):\n",
        "    \"\"\"\n",
        "    Get the prompt after T5 decoding to make sure dictionary\n",
        "    of prompts and summaries is consistent decode prompt from trlX pipeline\n",
        "    \"\"\"\n",
        "    formatted_prompts = []\n",
        "    for i in tqdm(range(len(prompts))):\n",
        "        tmp = tokenizer.decode(\n",
        "            tokenizer(\n",
        "                prompts[i].split(\"Summary: \")[0],\n",
        "                truncation=True,\n",
        "                max_length=max_length - 5,  # to make sure \"TL;DR\" dont get truncated\n",
        "                add_special_tokens=False,\n",
        "            )[\"input_ids\"],\n",
        "            skip_special_tokens=True,\n",
        "        ).strip()\n",
        "        tmp = tmp + \"\\nSummary: \"\n",
        "        tmp = tokenizer.decode(\n",
        "            tokenizer(tmp, truncation=True, max_length=max_length, add_special_tokens=False)[\"input_ids\"],\n",
        "            skip_special_tokens=True,\n",
        "        ).strip()\n",
        "        formatted_prompts.append(tmp)\n",
        "    return formatted_prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def remove_unwanted_char(row):\n",
        "    text = row['text']\n",
        "    pattern = re.compile(r'[^가-힣a-zA-Z0-9,.!?\\r\\t\\n\\f' + string.punctuation + ']')\n",
        "    return {'text': pattern.sub('', text)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not preprocessed_dataset_existance:\n",
        "    dataset_dict = dataset_dict.map(remove_unwanted_char)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not preprocessed_dataset_existance:\n",
        "    if not os.path.exists('data/pseudo_train_set.pkl'):\n",
        "        train_set = [sample[\"text\"] for sample in dataset_dict[\"train\"]]\n",
        "        train_set = get_prompt_dataset(train_set)\n",
        "        with open('data/pseudo_train_set.pkl', 'wb') as f:\n",
        "            pickle.dump(train_set, f)\n",
        "    else:\n",
        "        with open('data/pseudo_train_set.pkl', 'rb') as f:\n",
        "            train_set = pickle.load(f)\n",
        "\n",
        "    if not os.path.exists('data/pseudo_val_set.pkl'):\n",
        "        val_set = [sample[\"text\"] for sample in dataset_dict[\"test\"]]\n",
        "        val_set = get_prompt_dataset(val_set)\n",
        "        with open('data/pseudo_val_set.pkl', 'wb') as f:\n",
        "            pickle.dump(val_set, f)\n",
        "    else:\n",
        "        with open('data/pseudo_val_set.pkl', 'rb') as f:\n",
        "            val_set = pickle.load(f)\n",
        "else:\n",
        "    if not os.path.exists('data/train_set.pkl'):\n",
        "        train_set = [sample[\"text\"] for sample in dataset_dict[\"train\"]]\n",
        "        train_set = get_prompt_dataset(train_set)\n",
        "        with open('data/train_set.pkl', 'wb') as f:\n",
        "            pickle.dump(train_set, f)\n",
        "    else:\n",
        "        with open('data/train_set.pkl', 'rb') as f:\n",
        "            train_set = pickle.load(f)\n",
        "\n",
        "    if not os.path.exists('data/val_set.pkl'):\n",
        "        val_set = [sample[\"text\"] for sample in dataset_dict[\"test\"]]\n",
        "        val_set = get_prompt_dataset(val_set)\n",
        "        with open('data/val_set.pkl', 'wb') as f:\n",
        "            pickle.dump(val_set, f)\n",
        "    else:\n",
        "        with open('data/val_set.pkl', 'rb') as f:\n",
        "            val_set = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dzx3pZhnr2Co"
      },
      "outputs": [],
      "source": [
        "trainer = train(\n",
        "    prompts=train_set,\n",
        "    eval_prompts=val_set,\n",
        "    reward_fn=get_reward,\n",
        "    config=config,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer.save_pretrained(model_save_path)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
