{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "\n",
    "'Process in Colab' if IN_COLAB else 'Process in Local'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    !pip install transformers\n",
    "    !pip install datasets\n",
    "    !pip install --upgrade accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 깃허브에서는 빼야됨\n",
    "%cd drive/MyDrive/projects/ClauseSummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import datetime\n",
    "import re\n",
    "import os\n",
    "import gc\n",
    "from pprint import pprint\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.optim as optim\n",
    "from torch.optim import AdamW, SGD\n",
    "from torch.nn import MSELoss\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from datasets import load_from_disk, load_dataset, Dataset, DatasetDict, concatenate_datasets\n",
    "from transformers import AutoTokenizer, LongformerTokenizer, AutoModel, AutoModelForMaskedLM\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_newline_before_number(text:str) -> str: #숫자. 형태로 되어있는것에 개행문자를 추가.\n",
    "    text = re.sub(r'(\\d+)\\.',r'\\n\\1.', str(text))\n",
    "    return text\n",
    "\n",
    "def change_it_to_a_comma(text:str): # (1) (2) 형태를 ,으로\n",
    "    items = re.split(r'\\(\\d+\\)', text)\n",
    "    if len(items) > 1:\n",
    "        items[1] = items[0]+items[1]\n",
    "        del items[0]\n",
    "    return ','.join(items)\n",
    "\n",
    "def remove_whitespace_after_str(text:str):\n",
    "    text = re.sub(r\"\\b갑\\s\", r'갑', text)\n",
    "    text = re.sub(r\"\\b을\\s\", r'을', text)\n",
    "    text = re.sub(r\"\\b병\\s\", r'병', text)\n",
    "    text = re.sub(r\"\\b정\\s\", r'정', text)\n",
    "    return text\n",
    "\n",
    "def change_number_point(text:str): # 1. 2. 등을 제 1조 2 항 등으로 바꿔줌\n",
    "    items = re.split(r'\\d+\\.', text)\n",
    "    if len(items) > 1:\n",
    "        items[1] = items[0]+items[1]\n",
    "        del items[0]\n",
    "    return ''.join(items)\n",
    "\n",
    "def summary_preprocessing_func(text: str):\n",
    "    text = add_newline_before_number(text)\n",
    "    text = change_it_to_a_comma(text)\n",
    "    text = remove_whitespace_after_str(text)\n",
    "    text = change_number_point(text)\n",
    "    return text\n",
    "\n",
    "def text_preprocessing_func(text):\n",
    "    return re.sub(r'\\n[\\n ]+', '\\n', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(row: Dict[str, str]):\n",
    "    text = row['text']\n",
    "    summary = row['summary']\n",
    "    text = text_preprocessing_func(text)\n",
    "    summary = summary_preprocessing_func(summary)\n",
    "\n",
    "    return {'text': text, \n",
    "            'summary': summary\n",
    "            }\n",
    "\n",
    "def df_preprocessing(df: pd.DataFrame):\n",
    "    text_df = df[['text', 'summary']]\n",
    "    text_df = text_df.apply(preprocessing, axis=1, result_type='expand', )\n",
    "\n",
    "    df[['text', 'summary']] = text_df[['text', 'summary']]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizeMapWrapper:\n",
    "    def __init__(self, tokenizer, feature, option=None):\n",
    "        if option is None:\n",
    "            option = {\n",
    "                'max_length': 4096,\n",
    "                'truncation': True,\n",
    "                'padding': 'max_length',\n",
    "            }\n",
    "        \n",
    "        self.feature = feature\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __call__(self, row):\n",
    "        return self.tokenizer(row[self.feature], **self.option)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'{self.__class__.__name__}(tokenizer={self.tokenizer})'\n",
    "\n",
    "class RewardTokenizeMapWrapper(TokenizeMapWrapper):\n",
    "    def __init__(self, tokenizer, text_feature, summary_feature, max_token=4096, option=None):\n",
    "        if option is None:\n",
    "            option = {\n",
    "                'max_length': max_token,\n",
    "                'truncation': True,\n",
    "            }\n",
    "\n",
    "        self.max_token = option['max_length']\n",
    "        self.option = option\n",
    "        self.text_feature = text_feature\n",
    "        self.summary_feature = summary_feature\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __call__(self, row):\n",
    "        text = row[self.text_feature]\n",
    "        summary = row[self.summary_feature]\n",
    "        \n",
    "        tokenized_text = self.tokenizer(text, **self.option)\n",
    "        tokenized_summary = self.tokenizer(summary, **self.option)\n",
    "        tokenized_total_text = dict()\n",
    "        for key in tokenized_text:\n",
    "            if len(tokenized_text['input_ids']) + len(tokenized_summary['input_ids']) < self.max_token:\n",
    "                tokenized_total_text[key] = tokenized_text[key] + tokenized_summary[key]\n",
    "            else:\n",
    "                tokenized_total_text[key] = (tokenized_text[key][:- len(tokenized_summary['input_ids'])]\n",
    "                                             + tokenized_summary[key]\n",
    "                )\n",
    "            tokenized_total_text[key] = (tokenized_total_text[key] \n",
    "                                         + ([1] * (self.max_token - len(tokenized_total_text[key])))\n",
    "            )\n",
    "        return tokenized_total_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelForRewardGeneration(nn.Module):\n",
    "    def __init__(self, encoder_path, hidden_size=256):\n",
    "        super(ModelForRewardGeneration, self).__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(encoder_path)\n",
    "        self.hidden_size = hidden_size\n",
    "        # TODO: head 설계\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(768, hidden_size, bias=False),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout1d(0.1),\n",
    "            nn.Linear(hidden_size, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None):\n",
    "        x = self.encoder(input_ids, attention_mask).pooler_output\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "def reference_reward_loss(reward, pred):\n",
    "    return - torch.log10(1 + torch.exp(-reward * pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MANUAL_TRAINING = True\n",
    "MANUAL_VALIDATION = True\n",
    "MID_CHECKPOINT_NUM = 2\n",
    "MID_PROCESS_PRINT_NUM = 35\n",
    "\n",
    "NUM_EPOCHS = 1\n",
    "MAX_TOKEN = 4096\n",
    "learning_rate = 2e-5\n",
    "decay = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kolongformer_checkpoint = \"psyche/kolongformer-4096\"\n",
    "checkpoint = kolongformer_checkpoint\n",
    "print(f'Using Checkpoint: {checkpoint}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dataset_path = './data/dataset-term-reward.json'\n",
    "tokenized_dataset_path = f'./data/{checkpoint.replace(\"/\", \"-\")}-tokenized-dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_STR = datetime.datetime.now().strftime('%y%m%d-%H:%M')\n",
    "model_save_path = f\"./model/{SAVE_STR}\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Tokenizer & Model Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = LongformerTokenizer.from_pretrained(checkpoint)\n",
    "#tokenizer = LongformerTokenizer(vocab_file, merges_file, errors='replace', bos_token='<s>', eos_token='</s>', sep_token='</s>', cls_token='<s>', unk_token='<unk>', pad_token='<pad>', mask_token='<mask>', add_prefix_space=False, **kwargs)\n",
    "\n",
    "model = ModelForRewardGeneration(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(tokenizer) != model.encoder.config.vocab_size:\n",
    "    raise RuntimeError(f'Tokenizer vocab size and model vocab size do not match(Tokenizer:{len(tokenizer)} Model: {model.config.vocab_size}). Which would lead to further error in training.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(original_dataset_path)\n",
    "df['reward'] = df['reward'] / 10\n",
    "\n",
    "if not os.path.exists(tokenized_dataset_path):\n",
    "    text_df = df_preprocessing(df)\n",
    "    \n",
    "    dataset = Dataset.from_pandas(df[['text', 'summary', 'reward']])\n",
    "    tokenizer_wrapper = RewardTokenizeMapWrapper(tokenizer, 'text', 'summary')\n",
    "\n",
    "    tokenized_dataset = (dataset\n",
    "                         .map(tokenizer_wrapper)\n",
    "                         .remove_columns(['text', 'summary'])\n",
    "                         )\n",
    "    \n",
    "    tokenized_dataset.save_to_disk(tokenized_dataset_path)\n",
    "else:\n",
    "    tokenized_dataset = load_from_disk(tokenized_dataset_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## finding the best parameters\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "total_loss = []\n",
    "epoch_loss = []\n",
    "batch_loss = []\n",
    "\n",
    "model.train()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "trainset = tokenized_dataset.with_format('torch', device=device)\n",
    "dataloader = DataLoader(trainset, batch_size=12, shuffle=False) # TODO: Batch size 조절\n",
    "\n",
    "# TODO: Minor Hyperparameter Tuning\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=decay)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=100, num_training_steps=NUM_EPOCHS * len(dataloader))\n",
    "training_stats = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    total_steps = len(dataloader)\n",
    "    save_divisor = total_steps // MID_CHECKPOINT_NUM\n",
    "    print_divisor = total_steps // MID_PROCESS_PRINT_NUM\n",
    "    with tqdm(dataloader, leave=False, desc='Batch', position=0, postfix={'Epoch': 1, 'Batch': 1, 'loss': 0, 'loss_mean': 0}) as tqdm_bar:\n",
    "        for i, batch in enumerate(tqdm_bar):\n",
    "            tqdm_bar.set_description(f'Batch: {i + 1}')\n",
    "            X = {\n",
    "                    'input_ids': batch['input_ids'],\n",
    "                    'attention_mask': batch['attention_mask'],\n",
    "                }\n",
    "            y = batch['reward'].unsqueeze(0)\n",
    "\n",
    "            output = model(**X)\n",
    "            loss = criterion(output, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            scheduler.step()\n",
    "            batch_loss.append(loss.item())\n",
    "\n",
    "            if i % print_divisor == print_divisor - 1:\n",
    "                epoch_loss += batch_loss\n",
    "                batch_loss_series = pd.Series(batch_loss, dtype=np.float64)\n",
    "                training_stats.append(\n",
    "                    {\n",
    "                        'Epoch': epoch + 1,\n",
    "                        'Batch': i + 1,\n",
    "                        'loss': loss.item(),\n",
    "                        'loss_mean': batch_loss_series.mean()\n",
    "                    }\n",
    "                )\n",
    "                tqdm_bar.set_postfix(training_stats[-1])\n",
    "                batch_loss = []\n",
    "\n",
    "            if i % save_divisor == save_divisor - 1:\n",
    "                torch.save(model.state_dict(), f'{model_save_path}-{epoch}-{i}.pt')\n",
    "\n",
    "            total_loss += epoch_loss\n",
    "            batch_loss_series = pd.Series(epoch_loss, dtype=np.float64)\n",
    "            epoch_loss = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_stats_df = pd.DataFrame(training_stats)\n",
    "training_stats_df.to_csv('./training_stats.csv', index=False)\n",
    "\n",
    "torch.save(model.state_dict(), f'{model_save_path}-final.pt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_stats_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_loss = pd.Series(total_loss)\n",
    "total_loss.plot.line()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_loss.to_csv('./total_loss.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
